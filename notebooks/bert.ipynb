{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f71bff15-8f55-4662-8f52-061e4c55a1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1775c26-8107-459b-ba17-34f25689f4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.expanduser('~/develop/ClearML_ML_SD.yml'), 'r') as f:\n",
    "    keys = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf62f49c-02ac-4ad6-98df-aee7364eb514",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CLEARML_WEB_HOST\"] = \"https://app.clear.ml\"\n",
    "os.environ[\"CLEARML_API_HOST\"] = \"https://api.clear.ml\"\n",
    "os.environ[\"CLEARML_FILES_HOST\"] = \"https://files.clear.ml\"\n",
    "os.environ[\"CLEARML_API_ACCESS_KEY\"] = keys['access_key']\n",
    "os.environ[\"CLEARML_API_SECRET_KEY\"] = keys['secret_key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ca9c0cb-b811-4707-9305-fd7f89d5d970",
   "metadata": {},
   "outputs": [],
   "source": [
    "from clearml import Task, Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8adb8303-3d0b-41fe-8869-2b17ed97667a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Task: overwriting (reusing) task id=a80888e22dfb4a67b33be4ff8ae23846\n",
      "2022-10-25 15:02:11,974 - clearml.Repository Detection - WARNING - Failed accessing the jupyter server(s): []\n",
      "2022-10-25 15:02:11,989 - clearml.Task - INFO - No repository found, storing script code instead\n",
      "ClearML results page: https://app.clear.ml/projects/922c69dbd48249b183708fef50f18e10/experiments/a80888e22dfb4a67b33be4ff8ae23846/output/log\n",
      "ClearML Monitor: Could not detect iteration reporting, falling back to iterations as seconds-from-start\n"
     ]
    }
   ],
   "source": [
    "task = Task.init(\n",
    "    project_name='ML_SD', \n",
    "    task_name='bert', \n",
    "    tags=['bert'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4f840b4-dcbb-411b-8a17-10329c1cbd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics as skm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig\n",
    "# Adam с исправлениями и планировщик learning rate\n",
    "from transformers.optimization import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1425fce-d22b-4bf7-806c-81c2984181a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19e3d973-4d5f-4b82-8afe-ea9d2b3bc67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 21\n",
    "PATH = '../data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aca62047-b49f-4439-ae0e-be76d6bfc785",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(os.path.join(PATH, 'train.csv'))\n",
    "df_val = pd.read_csv(os.path.join(PATH, 'val.csv'))\n",
    "df_test = pd.read_csv(os.path.join(PATH, 'test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00c65758-59c0-4dc3-a98d-167781570a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_form(list_words, morph):\n",
    "    return [morph.parse(word)[0].normal_form for word in list_words]\n",
    "\n",
    "def del_stopwords(list_words, stop_words):\n",
    "    return [word for word in list_words if word not in stop_words]\n",
    "\n",
    "def transform_data(df):\n",
    "    df = df.copy()\n",
    "    df['level_2'] = df['icd10'].str.split('.').apply(lambda x: x[0])\n",
    "    df['level_1'] = df['icd10'].apply(lambda x: x[0])\n",
    "    # df['symptoms_tokens'] = df['symptoms'] \\\n",
    "    #     .str.lower() \\\n",
    "    #     .str.split('[^a-zа-яё]+') \\\n",
    "    #     .progress_apply(partial(norm_form, morph=MorphAnalyzer())) \\\n",
    "    #     .progress_apply(partial(del_stopwords, stop_words=get_stop_words('russian')))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11938a04-7a89-440e-b774-af6d559b3974",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = transform_data(df_train)\n",
    "df_val = transform_data(df_val)\n",
    "df_test = transform_data(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb0be633-bf4f-4dec-92ff-d7fd6225309d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask = df_train.groupby('level_2').transform('size') > 10\n",
    "# df_train = df_train[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40558b73-6e50-4a3f-b174-38affe550050",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68dcfd95-c215-460b-a0a7-2213bd455d61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symptoms</th>\n",
       "      <th>anamnesis</th>\n",
       "      <th>icd10</th>\n",
       "      <th>new_patient_id</th>\n",
       "      <th>new_event_id</th>\n",
       "      <th>new_event_time</th>\n",
       "      <th>level_2</th>\n",
       "      <th>level_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Состояние улучшилось, жалоб нет.</td>\n",
       "      <td>Принимала - ничего ФЛГ от *ДАТА* - без патолог...</td>\n",
       "      <td>J00</td>\n",
       "      <td>q30c3b31</td>\n",
       "      <td>qad30faf</td>\n",
       "      <td>2027-01-16</td>\n",
       "      <td>J00</td>\n",
       "      <td>J</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49лет, активно жалоб не предъявляет Пришла с р...</td>\n",
       "      <td>цикл нерегулярный. Принимает ци-клим непрерывн...</td>\n",
       "      <td>D25.1</td>\n",
       "      <td>q56209d0</td>\n",
       "      <td>q9c869f7</td>\n",
       "      <td>2022-12-09</td>\n",
       "      <td>D25</td>\n",
       "      <td>D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>появление пигиентных пятен на лице прибавка ве...</td>\n",
       "      <td>Вышеперечисленные жалобы с *ДАТА* ( с периода ...</td>\n",
       "      <td>E04.1</td>\n",
       "      <td>q599e008</td>\n",
       "      <td>q1e98632</td>\n",
       "      <td>2028-11-26</td>\n",
       "      <td>E04</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Дискомфорт в области верх трети шеи, эпизодиче...</td>\n",
       "      <td>Множественная миома матки, паратубарная киста ...</td>\n",
       "      <td>E06.3</td>\n",
       "      <td>qd92e2f1</td>\n",
       "      <td>q6ad490c</td>\n",
       "      <td>2024-01-07</td>\n",
       "      <td>E06</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Состояние без изменений Жалобы при первичном о...</td>\n",
       "      <td>Впервые появились боли после подъема тяжесьти,...</td>\n",
       "      <td>K21.0</td>\n",
       "      <td>qc286856</td>\n",
       "      <td>q22bea7e</td>\n",
       "      <td>2024-06-21</td>\n",
       "      <td>K21</td>\n",
       "      <td>K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            symptoms  \\\n",
       "0                   Состояние улучшилось, жалоб нет.   \n",
       "1  49лет, активно жалоб не предъявляет Пришла с р...   \n",
       "2  появление пигиентных пятен на лице прибавка ве...   \n",
       "3  Дискомфорт в области верх трети шеи, эпизодиче...   \n",
       "4  Состояние без изменений Жалобы при первичном о...   \n",
       "\n",
       "                                           anamnesis  icd10 new_patient_id  \\\n",
       "0  Принимала - ничего ФЛГ от *ДАТА* - без патолог...    J00       q30c3b31   \n",
       "1  цикл нерегулярный. Принимает ци-клим непрерывн...  D25.1       q56209d0   \n",
       "2  Вышеперечисленные жалобы с *ДАТА* ( с периода ...  E04.1       q599e008   \n",
       "3  Множественная миома матки, паратубарная киста ...  E06.3       qd92e2f1   \n",
       "4  Впервые появились боли после подъема тяжесьти,...  K21.0       qc286856   \n",
       "\n",
       "  new_event_id new_event_time level_2 level_1  \n",
       "0     qad30faf     2027-01-16     J00       J  \n",
       "1     q9c869f7     2022-12-09     D25       D  \n",
       "2     q1e98632     2028-11-26     E04       E  \n",
       "3     q6ad490c     2024-01-07     E06       E  \n",
       "4     q22bea7e     2024-06-21     K21       K  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5998906a-4fa3-44a0-aace-c74069637ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('cointegrated/rubert-tiny2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de9aae09-7556-4ab2-8b1f-08e0d889ffc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_comments_to_tensors(comments):\n",
    "    features = []\n",
    "    for comment in comments:\n",
    "        # full preparation for input to BERT model, including BPE-encoding,\n",
    "        # converting tokens to ids, padding, adding special tokens in the beginning and end of a sequence \n",
    "        items = tokenizer.encode_plus(\n",
    "            comment, \n",
    "            max_length=100, \n",
    "            truncation=True, \n",
    "            add_special_tokens=True, \n",
    "            pad_to_max_length=True\n",
    "        )\n",
    "        features.append(items)\n",
    "\n",
    "    input_ids = torch.tensor([f['input_ids'] for f in features], dtype=torch.long)\n",
    "    # a mask, it has 1 - where a token exists and 0 where it's a padding index\n",
    "    attention_mask = torch.tensor([f['attention_mask'] for f in features], dtype=torch.long)\n",
    "    return input_ids, attention_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e03ee92a-55d7-4f26-90df-79118fa2896f",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_col = 'level_2'\n",
    "text_col = 'symptoms'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "daa205cf-88ac-45c8-b72f-647a14e34b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dima/anaconda3/envs/torch/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2302: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5604/1010/1011 - train/val/test split\n"
     ]
    }
   ],
   "source": [
    "X_train = convert_comments_to_tensors(df_train[text_col].values)\n",
    "X_val = convert_comments_to_tensors(df_val[text_col].values)\n",
    "X_test = convert_comments_to_tensors(df_test[text_col].values)\n",
    "\n",
    "i2l = dict(enumerate(sorted(df_train['level_2'].unique())))\n",
    "l2i = {label: i for i, label in i2l.items()}\n",
    "\n",
    "y_train = df_train['level_2'].map(l2i).values\n",
    "y_val = df_val['level_2'].map(l2i)\n",
    "y_val = y_val.fillna(list(set(y_train) - set(y_val))[0]).values\n",
    "y_test = df_test['level_2'].map(l2i)\n",
    "y_test = y_test.fillna(list(set(y_train) - set(y_test))[0]).values\n",
    "\n",
    "print(\"{}/{}/{} - train/val/test split\".format(y_train.shape[0], y_val.shape[0], y_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a606251-4eac-494d-bd50-196e6afa7a8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[    2, 46039, 75281,  ...,     0,     0,     0],\n",
       "         [    2,  1562, 11273,  ...,     0,     0,     0],\n",
       "         [    2, 32279, 51393,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [    2, 35282,   626,  ...,     0,     0,     0],\n",
       "         [    2,    17, 70991,  ...,     0,     0,     0],\n",
       "         [    2,   548, 33073,  ...,     0,     0,     0]]),\n",
       " tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a79e915-681b-4286-bf70-91789266b8b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([185,  32,  52, ..., 353,  50, 196])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5942abb1-10d3-4311-8fd2-3fa2699570c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-25 15:02:30,348 - clearml.model - INFO - Selected model id: 748b4796efdf4dd5812a028ad474e0bb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cointegrated/rubert-tiny2 were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(83828, 312, padding_idx=0)\n",
       "      (position_embeddings): Embedding(2048, 312)\n",
       "      (token_type_embeddings): Embedding(2, 312)\n",
       "      (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (key): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (value): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=312, out_features=600, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=600, out_features=312, bias=True)\n",
       "            (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (key): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (value): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=312, out_features=600, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=600, out_features=312, bias=True)\n",
       "            (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (key): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (value): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "              (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=312, out_features=600, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=600, out_features=312, bias=True)\n",
       "            (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=312, out_features=312, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=312, out_features=436, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "config = BertConfig.from_pretrained(\n",
    "    'cointegrated/rubert-tiny2', # bert-base-uncased\n",
    "    num_labels=len(set(y_train))\n",
    ")\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'cointegrated/rubert-tiny2',\n",
    "    from_tf=False,\n",
    "    config=config\n",
    ")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "809c0d52-253e-4c31-9def-a33fd8622045",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_loader):\n",
    "    total_loss = 0.\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    model.eval()  # Set mode to evaluation to disable dropout & freeze BN\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(data_loader):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            inputs = {\n",
    "                'input_ids': batch[0],\n",
    "                'attention_mask': batch[1],\n",
    "                'labels': batch[2]\n",
    "            }\n",
    "            outputs = model(**inputs)\n",
    "            total_loss += outputs[0]\n",
    "            y_pred.extend(outputs[1].cpu().numpy())\n",
    "            y_true.extend(batch[2].cpu().numpy())\n",
    "\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    y_true = np.asarray(y_true)\n",
    "\n",
    "    results = {\n",
    "        'val_hit3': hit_at_n(y_true, y_pred, n=3), \n",
    "        'val_precision': hit_at_n(y_true, y_pred, n=1), \n",
    "        'val_loss': total_loss / len(data_loader)\n",
    "    }\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Identify whether metric has not been improved for certain number of epochs\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 mode: str = 'min',\n",
    "                 min_delta: float = 0,\n",
    "                 patience: int = 10):\n",
    "        self.mode = mode\n",
    "        self.min_delta = min_delta\n",
    "        self.patience = patience\n",
    "\n",
    "        self.is_better = None\n",
    "        if patience == 0:\n",
    "            self.is_better = lambda *_: True\n",
    "        else:\n",
    "            self._init_is_better(mode, min_delta)\n",
    "\n",
    "        self.best = None\n",
    "        self.num_bad_epochs = 0\n",
    "\n",
    "    def step(self, current) -> bool:\n",
    "        \"\"\"\n",
    "        Make decision whether to stop training\n",
    "\n",
    "        :param current: new metric value\n",
    "        :return: whether to stop\n",
    "        \"\"\"\n",
    "        if isinstance(current, torch.Tensor):\n",
    "            current = current.cpu()\n",
    "        if np.isnan(current):\n",
    "            return True\n",
    "\n",
    "        if self.best is None:\n",
    "            self.best = current\n",
    "        else:\n",
    "            if self.is_better(current, self.best):\n",
    "                self.num_bad_epochs = 0\n",
    "                self.best = current\n",
    "            else:\n",
    "                self.num_bad_epochs += 1\n",
    "\n",
    "        if self.num_bad_epochs >= self.patience:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def _init_is_better(self, mode, min_delta):\n",
    "        if mode not in {'min', 'max'}:\n",
    "            raise ValueError('mode ' + mode + ' is unknown!')\n",
    "        if mode == 'min':\n",
    "            self.is_better = lambda value, best: value < best - min_delta\n",
    "        if mode == 'max':\n",
    "            self.is_better = lambda value, best: value > best + min_delta\n",
    "            \n",
    "            \n",
    "class ModelCheckpoint:\n",
    "    \"\"\"Save the model after every epoch.\n",
    "    `filepath` can contain named formatting options,\n",
    "    which will be filled the value of `epoch` and `val_loss`.\n",
    "    For example: if `filepath` is `weights.{epoch:02d}-{val_loss:.2f}.hdf5`,\n",
    "    then the model checkpoints will be saved with the epoch number and\n",
    "    the validation loss in the filename.\n",
    "    # Arguments\n",
    "        model: PyTorch model object\n",
    "        filepath: string, path to save the model file.\n",
    "        save_best_only: if `save_best_only=True`,\n",
    "            the latest best model according to\n",
    "            the quantity monitored will not be overwritten.\n",
    "        mode: one of {min, max}.\n",
    "            If `save_best_only=True`, the decision\n",
    "            to overwrite the current save file is made\n",
    "            based on either the maximization or the\n",
    "            minimization of the monitored quantity. For `val_acc`,\n",
    "            this should be `max`, for `val_loss` this should\n",
    "            be `min`, etc.\n",
    "        save_weights_only: if True, then only the model's weights will be\n",
    "            saved, else the full model is saved.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: torch.nn.Module,\n",
    "        filepath: str,\n",
    "        mode: str = \"min\",\n",
    "        save_best_only: bool = True,\n",
    "        save_weights_only: bool = False,\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.filepath = filepath\n",
    "        self.mode = mode\n",
    "        self.save_best_only = save_best_only\n",
    "        self.save_weights_only = save_weights_only\n",
    "        self.num_saves = 0\n",
    "\n",
    "        if mode == \"min\":\n",
    "            self.monitor_op = np.less\n",
    "            self.best = np.Inf\n",
    "        elif mode == \"max\":\n",
    "            self.monitor_op = np.greater\n",
    "            self.best = -np.Inf\n",
    "        else:\n",
    "            raise ValueError(\"mode \" + mode + \" is unknown!\")\n",
    "\n",
    "        Path(self.filepath).parent.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    def _save_model(self):\n",
    "        if self.save_weights_only:\n",
    "            torch.save(self.model.state_dict(), self.filepath)\n",
    "        else:\n",
    "            torch.save(self.model, self.filepath)\n",
    "        self.num_saves += 1\n",
    "\n",
    "    def step(self, current, epoch=None):\n",
    "        if isinstance(current, torch.Tensor):\n",
    "            current = current.cpu()\n",
    "        if self.save_best_only:\n",
    "            if self.monitor_op(current, self.best):\n",
    "                self.best = current\n",
    "                self._save_model()\n",
    "        else:\n",
    "            self._save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0fa2ade5-3def-4367-bfb6-69ddf7a481ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dima/anaconda3/envs/torch/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "lr = 0.0000125  # usually from 1e-5 until 8e-5\n",
    "warmup_steps = 50\n",
    "num_steps = 12000\n",
    "\n",
    "optimizer = AdamW([p for p in model.parameters() if p.requires_grad],\n",
    "                   lr=lr, weight_decay=0)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=num_steps)\n",
    "\n",
    "early_stopping = EarlyStopping(patience=8, mode='max')\n",
    "model_checkpoint = ModelCheckpoint(model, 'models/cointegrated_rubert_tiny2.pt', mode=\"max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "71d31f65-b8f8-4872-8621-363fc02d9a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of epochs: 1201\n"
     ]
    }
   ],
   "source": [
    "batch_size = 512\n",
    "gradient_accumulation_steps = 1\n",
    "logging_steps = 100  # периодичность проверки качества модели, чтобы во время остановить обучение\n",
    "max_grad_norm = 1\n",
    "\n",
    "train_dataset = TensorDataset(X_train[0], X_train[1], torch.LongTensor(y_train))\n",
    "val_dataset = TensorDataset(X_val[0], X_val[1], torch.LongTensor(y_val))\n",
    "test_dataset = TensorDataset(X_test[0], X_test[1], torch.LongTensor(y_test))\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "num_train_epochs = num_steps // (len(train_dataloader) // gradient_accumulation_steps) + 1\n",
    "global_step = 0\n",
    "tr_loss, logging_loss = 0.0, 0.0\n",
    "print('Count of epochs: %s' % num_train_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a1de6a82-dd7a-499f-b423-6a626324db43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hit_at_n(y_true, y_pred, n=3):\n",
    "    assert len(y_true) == len(y_pred)\n",
    "    \n",
    "    score = np.mean(np.any(\n",
    "        np.argsort(-y_pred, axis=1)[:, :n] == y_true.reshape(-1,1), \n",
    "        axis=1\n",
    "    ))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060d6b04-81da-4533-9941-81d666f6af91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 100, val_hit3: 0.1762 val_precision: 0.0980 val_loss: 5.7553 train_loss: 5.9687\n",
      "2022-10-25 15:03:06,053 - clearml.frameworks - INFO - Found existing registered model id=c34ee1e5e1964e1886b01ec9b6f3733d [/home/dima/files/projects/med/notebooks/models/cointegrated_rubert_tiny2.pt] reusing it.\n",
      "Step 200, val_hit3: 0.1772 val_precision: 0.0980 val_loss: 5.3528 train_loss: 5.5481\n",
      "Step 300, val_hit3: 0.1802 val_precision: 0.0980 val_loss: 5.0640 train_loss: 5.1962\n",
      "Step 400, val_hit3: 0.2089 val_precision: 0.1158 val_loss: 4.8840 train_loss: 4.9538\n",
      "Step 500, val_hit3: 0.2287 val_precision: 0.1386 val_loss: 4.7361 train_loss: 4.7917\n",
      "Step 600, val_hit3: 0.2752 val_precision: 0.1416 val_loss: 4.5984 train_loss: 4.6331\n",
      "Step 700, val_hit3: 0.2970 val_precision: 0.1634 val_loss: 4.4689 train_loss: 4.4822\n",
      "Step 800, val_hit3: 0.3168 val_precision: 0.1733 val_loss: 4.3581 train_loss: 4.3312\n",
      "Step 900, val_hit3: 0.3347 val_precision: 0.1911 val_loss: 4.2544 train_loss: 4.1859\n",
      "Step 1000, val_hit3: 0.3535 val_precision: 0.2099 val_loss: 4.1647 train_loss: 4.0540\n",
      "Step 1100, val_hit3: 0.3762 val_precision: 0.2337 val_loss: 4.0804 train_loss: 3.9288\n",
      "Step 1200, val_hit3: 0.3871 val_precision: 0.2436 val_loss: 4.0033 train_loss: 3.8016\n",
      "Step 1300, val_hit3: 0.3970 val_precision: 0.2505 val_loss: 3.9353 train_loss: 3.6920\n",
      "Step 1400, val_hit3: 0.4089 val_precision: 0.2554 val_loss: 3.8738 train_loss: 3.5784\n",
      "Step 1500, val_hit3: 0.4208 val_precision: 0.2614 val_loss: 3.8104 train_loss: 3.4792\n",
      "Step 1600, val_hit3: 0.4356 val_precision: 0.2733 val_loss: 3.7667 train_loss: 3.3825\n",
      "Step 1700, val_hit3: 0.4396 val_precision: 0.2733 val_loss: 3.7145 train_loss: 3.2933\n",
      "Step 1800, val_hit3: 0.4465 val_precision: 0.2792 val_loss: 3.6658 train_loss: 3.1963\n",
      "Step 1900, val_hit3: 0.4525 val_precision: 0.2822 val_loss: 3.6314 train_loss: 3.1234\n",
      "Step 2000, val_hit3: 0.4515 val_precision: 0.2842 val_loss: 3.5992 train_loss: 3.0386\n",
      "Step 2100, val_hit3: 0.4515 val_precision: 0.2861 val_loss: 3.5567 train_loss: 2.9688\n",
      "Step 2200, val_hit3: 0.4604 val_precision: 0.2891 val_loss: 3.5369 train_loss: 2.8987\n"
     ]
    }
   ],
   "source": [
    "for _ in range(num_train_epochs):\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        model.train()\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        inputs = {\n",
    "            'input_ids': batch[0],\n",
    "            'attention_mask': batch[1],\n",
    "            'labels': batch[2]\n",
    "        }\n",
    "        outputs = model(**inputs) # model outputs are tuple: (loss, logits)\n",
    "        loss = outputs[0]\n",
    "\n",
    "        if gradient_accumulation_steps > 1:\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "\n",
    "        tr_loss += loss.item()\n",
    "        if (step + 1) % gradient_accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            scheduler.step()  # Update learning rate schedule\n",
    "            model.zero_grad()\n",
    "            global_step += 1\n",
    "\n",
    "            # Log metrics\n",
    "            if global_step % logging_steps == 0:\n",
    "                results = evaluate(val_dataloader)\n",
    "                results.update({'train_loss': (tr_loss - logging_loss) / logging_steps})\n",
    "                print('Step {:3}, {}'.format(global_step, ' '.join(['{}: {:<6.4f}'.format(k, v) for k, v in\n",
    "                                                                  results.items()])))\n",
    "                logging_loss = tr_loss\n",
    "\n",
    "                # Saving model checkpoint here if we have improvement\n",
    "                model_checkpoint.step(results[\"val_hit3\"])\n",
    "\n",
    "                if early_stopping.step(results['val_hit3']):\n",
    "                    global_step = num_steps + 1\n",
    "                    print('Early training stopping!')\n",
    "                    break\n",
    "\n",
    "    if global_step > num_steps:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef0b1ea-b874-4002-9cb8-0480a7aace42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bfdc0a-3d7a-455c-8686-43fe3dad1e08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f6b70e-cfaa-47aa-bae1-8a08b92599ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b38335e-8abc-4d1e-968f-4f601fa80853",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0144e94d-c1fc-4866-97c0-7238a59b62ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8230a5-f3e2-4a22-87ec-8243687d3530",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60149d0b-e707-434a-8016-20e962af7b51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
