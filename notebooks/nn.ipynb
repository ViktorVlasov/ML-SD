{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dea7db4-e56f-4e11-a6c4-b327fff0af15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e50989a6-34b7-4159-a4da-ec226e1f2340",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import os\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from functools import partial\n",
    "from stop_words import get_stop_words\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "SEED = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18d5d4aa-cf75-45d6-9c6e-098638e7d9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '../data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d9e3f4b-083b-486b-b17e-559116666eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(os.path.join(PATH, 'train.csv'))\n",
    "df_val = pd.read_csv(os.path.join(PATH, 'val.csv'))\n",
    "df_test = pd.read_csv(os.path.join(PATH, 'test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff6b7571-c70d-4faa-8c12-3107a3d73a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_form(list_words, morph):\n",
    "    return [morph.parse(word)[0].normal_form for word in list_words]\n",
    "\n",
    "def del_stopwords(list_words, stop_words):\n",
    "    return [word for word in list_words if word not in stop_words]\n",
    "\n",
    "def transform_data(df):\n",
    "    df = df.copy()\n",
    "    df['level_2'] = df['icd10'].str.split('.').apply(lambda x: x[0])\n",
    "    df['level_1'] = df['icd10'].apply(lambda x: x[0])\n",
    "    df['symptoms_tokens'] = df['symptoms'] \\\n",
    "        .str.lower() \\\n",
    "        .str.split('[^a-zа-яё]+') \\\n",
    "        .progress_apply(partial(norm_form, morph=MorphAnalyzer())) \\\n",
    "        .progress_apply(partial(del_stopwords, stop_words=get_stop_words('russian')))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c72ef18e-a48d-4692-8818-dfab6af37d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetEncoder:\n",
    "    def __init__(self):\n",
    "        self.i2l = None\n",
    "        self.l2i = None\n",
    "    \n",
    "    def fit(self, y):\n",
    "        self.i2l = dict(enumerate(sorted(set(y))))\n",
    "        self.l2i = {label: i for i, label in self.i2l.items()}\n",
    "        \n",
    "    def transform(self, y):\n",
    "        default_index = max(self.l2i.values()) + 1\n",
    "        label = np.array([\n",
    "            self.l2i[label] if label in self.l2i else default_index \n",
    "            for label in y\n",
    "        ])\n",
    "        \n",
    "    def inverse_transforn(self, y):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df2a0f3-1ac0-4ff5-b084-8436146a80ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd280b4-1a83-4f4b-8e72-779ad1b7becb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e369bab6-fa16-4cd0-8476-8dd6e267f09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edca212a-ea29-4146-9b42-8b0cd28969c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff7f3908-ae94-40b8-98d4-faf6b2997ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 3\n",
    "\n",
    "\n",
    "# level_2 = cats_level_2[i]\n",
    "# level_1 = dict_levels[level_2]\n",
    "# idx = dict_level2idx[level_1]\n",
    "# idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57333188-6273-4265-857a-d5ff7ad90494",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6999bcb5-8993-492b-8c42-823c14607db7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d262ac7f24c84728b6c06aa1ff4a6bc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5604 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1ea363416d444f197a5e42a8f8b3844",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5604 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a279b15e4d2544f5b4ddbcd376e821d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1010 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69b8f7dbb1214fcea5fc47ae98b0d38d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1010 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0ab20a8f2ce445ca7219b12fff66d62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1011 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89307b4c9eb447358c8b4d067ceccec6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1011 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train = transform_data(df_train)\n",
    "X_val = transform_data(df_val)\n",
    "X_test = transform_data(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "365631f0-6eba-496a-9977-376084a97afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_enc_1 = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "target_enc_1.fit(X_train[['level_1']])\n",
    "\n",
    "target_enc_2 = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "target_enc_2.fit(X_train[['level_2']])\n",
    "\n",
    "cats_level_1 = target_enc_1.categories_[0]\n",
    "cats_level_2 = target_enc_2.categories_[0]\n",
    "\n",
    "dict_levels = dict(zip(*X_train[['level_2', 'level_1']].drop_duplicates().T.values))\n",
    "dict_level2idx = {cat: i for i, cat in enumerate(cats_level_1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af28e0d3-4314-4f4f-a491-3799785ba093",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb70d532-fe12-42ac-a85e-4f7bc930804e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dima/anaconda3/envs/torch/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:524: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer=lambda x: x, lowercase=False)\n",
    "tfidf.fit(X_train['symptoms_tokens'])\n",
    "\n",
    "X_train_new = tfidf.transform(X_train['symptoms_tokens']).toarray()\n",
    "X_val_new = tfidf.transform(X_val['symptoms_tokens']).toarray()\n",
    "X_test_new = tfidf.transform(X_test['symptoms_tokens']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d7613cf-b62a-46e9-b051-13d481e8a4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_1 = target_enc_1.transform(X_train[['level_1']])\n",
    "y_train_2 = target_enc_2.transform(X_train[['level_2']])\n",
    "\n",
    "y_val_1 = target_enc_1.transform(X_val[['level_1']])\n",
    "y_val_2 = target_enc_2.transform(X_val[['level_2']])\n",
    "\n",
    "y_test_1 = target_enc_1.transform(X_test[['level_1']])\n",
    "y_test_2 = target_enc_2.transform(X_test[['level_2']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74650da7-c039-41d7-b3d6-88d2f82a617f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, X, y1, y2):\n",
    "        self.X = X\n",
    "        self.y1 = y1\n",
    "        self.y2 = y2\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'X': torch.FloatTensor(self.X[idx]), \n",
    "            'y1': torch.FloatTensor(self.y1[idx]),\n",
    "            'y2': torch.FloatTensor(self.y2[idx]),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7744902-50b9-4c52-a9c6-349b20940883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ModelClassification(nn.Module):\n",
    "#     def __init__(self, num_feature, num_classes):\n",
    "#         super(ModelClassification, self).__init__()\n",
    "#         self.num_classes = num_classes\n",
    "#         self.fc1 = nn.Linear(num_feature, 512) # 12 is the number of features\n",
    "#         self.fc2 = nn.Linear(512, 256)\n",
    "#         self.fc3 = nn.Linear(256, 128)\n",
    "#         self.fc4 = nn.Linear(128, 64)\n",
    "        \n",
    "#         self.out = nn.Linear(64, 1)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = F.relu(self.fc2(x)) \n",
    "#         x = F.relu(self.fc3(x))\n",
    "#         x = F.relu(self.fc4(x))\n",
    "        \n",
    "#         out = torch.hstack([torch.sigmoid(self.out(x)) for i in range(self.num_classes)])\n",
    "                \n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "c2a3d5a0-b996-4092-8cb0-69f93bab607f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "005f084f-2cca-4007-b6d3-a8fc0e89b975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def binary_loss_fn(outputs, targets):\n",
    "    \n",
    "#     first = True\n",
    "#     for i in range(targets.shape[1]):\n",
    "#         o = outputs[:, i]\n",
    "#         t = targets[:, i]\n",
    "#         if first:\n",
    "#             loss = nn.BCELoss()(o, t)\n",
    "#             first = False\n",
    "#         else:\n",
    "#             loss += nn.BCELoss()(o, t)\n",
    "            \n",
    "#     loss /= len(targets)\n",
    "    \n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "6b039da8-c919-48ca-8d3e-eb39aa8cb4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hit_at_n(y_true, y_pred, n=3):\n",
    "    assert len(y_true) == len(y_pred)\n",
    "    \n",
    "    score = np.mean(np.any(\n",
    "        np.argsort(-y_pred, axis=1)[:, :n] == y_true.reshape(-1,1), \n",
    "        axis=1\n",
    "    ))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "2e11bf28-e9b9-470f-aa2a-af22f9a0ff97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "d9e96791-6815-4a51-ba9d-a564f9d55f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# кастомные функции для обучения моделей\n",
    "import gc\n",
    "\n",
    "def clear_cache():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "class Trainer():\n",
    "    def __init__(self, model, loss_func, opt, device='cpu'):\n",
    "        self.model = model.to(device)\n",
    "        self.loss_func = loss_func\n",
    "        self.opt = opt\n",
    "        self.device = device\n",
    "            \n",
    "    def train_epoch(self, train_iter, epoch):\n",
    "        loss_value = 0.0\n",
    "        \n",
    "        y1 = []\n",
    "        y2 = []\n",
    "        y_pred1 = []\n",
    "        y_pred2 = []\n",
    "        self.model.train()\n",
    "        pbar = tqdm(enumerate(train_iter), total=len(train_iter), leave=False)\n",
    "        pbar.set_description(f\"Epoch {epoch}\")\n",
    "        for it, batch in pbar: \n",
    "            self.opt.zero_grad()\n",
    "            \n",
    "            X = batch['X'].to(self.device)\n",
    "            y = batch['y1'].to(self.device), batch['y2'].to(self.device)\n",
    "            outputs = self.model(X)\n",
    "\n",
    "            loss = self.loss_func(outputs, y)\n",
    "            loss.backward()\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), 20)\n",
    "            self.opt.step()\n",
    "\n",
    "            loss_value += loss.item()\n",
    "\n",
    "            y1.append(batch['y1'].numpy().argmax(axis=1))\n",
    "            y2.append(batch['y2'].numpy().argmax(axis=1))\n",
    "\n",
    "            y_pred1.append(outputs[0].cpu().detach().numpy())\n",
    "            y_pred2.append(outputs[1].cpu().detach().numpy())\n",
    "\n",
    "            pbar.set_description(f\"\"\"\n",
    "                Train Loss: {loss:.4}\n",
    "            \"\"\")\n",
    "\n",
    "        y1 = np.hstack(y1)\n",
    "        y2 = np.hstack(y2)\n",
    "\n",
    "        y_pred1 = np.vstack(y_pred1)\n",
    "        y_pred2 = np.vstack(y_pred2)\n",
    "        \n",
    "        metrics = dict(\n",
    "            loss = loss_value / len(train_iter),\n",
    "            hit3_label1 = hit_at_n(y1, y_pred1, n=3),\n",
    "            hit3_label2 = hit_at_n(y2, y_pred2, n=3),\n",
    "            precision_label1 = hit_at_n(y1, y_pred1, n=1),\n",
    "            precision_label2 = hit_at_n(y2, y_pred2, n=1),\n",
    "        )\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "\n",
    "    def eval_epoch(self, val_iter, epoch):\n",
    "        loss_value = 0.0\n",
    "        \n",
    "        y1 = []\n",
    "        y2 = []\n",
    "        y_pred1 = []\n",
    "        y_pred2 = []\n",
    "        \n",
    "        self.model.eval()\n",
    "        pbar = tqdm(enumerate(val_iter), total=len(val_iter), leave=False)\n",
    "        pbar.set_description(f\"Epoch {epoch}\")\n",
    "        with torch.no_grad():\n",
    "            for it, batch in pbar:\n",
    "                X = batch['X'].to(self.device)\n",
    "                y = batch['y1'].to(self.device), batch['y2'].to(self.device)\n",
    "                outputs = self.model(X)\n",
    "                loss = self.loss_func(outputs, y)\n",
    "                loss_value += loss.item()\n",
    "\n",
    "                y1.append(batch['y1'].numpy().argmax(axis=1))\n",
    "                y2.append(batch['y2'].numpy().argmax(axis=1))\n",
    "\n",
    "                y_pred1.append(outputs[0].cpu().detach().numpy())\n",
    "                y_pred2.append(outputs[1].cpu().detach().numpy())\n",
    "                \n",
    "                pbar.set_description(f\"\"\"\n",
    "                    Test Loss: {loss:.4}\n",
    "                \"\"\")\n",
    "\n",
    "        y1 = np.hstack(y1)\n",
    "        y2 = np.hstack(y2)\n",
    "\n",
    "        y_pred1 = np.vstack(y_pred1)\n",
    "        y_pred2 = np.vstack(y_pred2)\n",
    "        \n",
    "        metrics = dict(\n",
    "            loss = loss_value / len(val_iter),\n",
    "            hit3_label1 = hit_at_n(y1, y_pred1, n=3),\n",
    "            hit3_label2 = hit_at_n(y2, y_pred2, n=3),\n",
    "            precision_label1 = hit_at_n(y1, y_pred1, n=1),\n",
    "            precision_label2 = hit_at_n(y2, y_pred2, n=1),\n",
    "        )\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "\n",
    "    def train_loop(self, train_iter, valid_iter, max_epochs, patience):\n",
    "\n",
    "        min_loss = np.inf\n",
    "\n",
    "        cur_patience = 0\n",
    "\n",
    "        for epoch in range(1, max_epochs + 1):\n",
    "            train_metrics = self.train_epoch(train_iter, epoch)\n",
    "            clear_cache()\n",
    "            \n",
    "            val_metrics = self.eval_epoch(valid_iter, epoch)\n",
    "            clear_cache()\n",
    "            \n",
    "            val_loss = val_metrics['loss']\n",
    "            if val_loss < min_loss:\n",
    "                min_loss = val_loss\n",
    "                best_model = self.model.state_dict()\n",
    "            else:\n",
    "                cur_patience += 1\n",
    "                if cur_patience == patience:\n",
    "                    cur_patience = 0\n",
    "                    break\n",
    "            clear_output()\n",
    "            print('%20s: %2d' % ('epoch', epoch))\n",
    "            print()\n",
    "            print('%20s: %7.4f %3.4f' % ('loss', train_metrics['loss'], val_metrics['loss']))\n",
    "            print()\n",
    "            print('%20s: %7.4f %3.4f' % ('hit3_label1', train_metrics['hit3_label1'], val_metrics['hit3_label1']))\n",
    "            print('%20s: %7.4f %3.4f' % ('precision_label1', train_metrics['precision_label1'], val_metrics['precision_label1']))\n",
    "            print()\n",
    "            print('%20s: %7.4f %3.4f' % ('hit3_label2', train_metrics['hit3_label2'], val_metrics['hit3_label2']))\n",
    "            print('%20s: %7.4f %3.4f' % ('precision_label2', train_metrics['precision_label2'], val_metrics['precision_label2']))\n",
    "\n",
    "#             print(*[f'{k}: {v}' for k, v in train_metrics.items()])\n",
    "#             print(*[f'{k}: {v}' for k, v in val_metrics.items()])\n",
    "\n",
    "        self.model.load_state_dict(best_model)\n",
    "        \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "9bfe3bbf-1a89-41f7-8ba5-dcdfb8e1baa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelClassification(nn.Module):\n",
    "    def __init__(self, num_feature, num_classes_1=len(cats_level_1), num_classes_2=len(cats_level_2)):\n",
    "        super(ModelClassification, self).__init__()\n",
    "        \n",
    "#         self.fc1 = nn.Linear(num_feature, 256)\n",
    "#         self.fc2 = nn.Linear(256, 128)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "#         self.dropout = nn.Dropout(p=0.2)\n",
    "#         self.batchnorm1 = nn.BatchNorm1d(256)\n",
    "#         self.batchnorm2 = nn.BatchNorm1d(128)\n",
    "        \n",
    "        self.out_2 = nn.Linear(num_feature, num_classes_2)\n",
    "        self.out_1 = nn.Linear(num_feature, num_classes_1)\n",
    "        self.fc = nn.Linear(num_classes_1, num_classes_2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "#         x = self.fc1(x)\n",
    "#         x = self.batchnorm1(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.dropout(x)\n",
    "        \n",
    "#         x = self.fc2(x)\n",
    "#         x = self.batchnorm2(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.dropout(x)\n",
    "        #x = F.tanh(self.fc2(x))\n",
    "        #x = F.tanh(self.fc3(x))\n",
    "        \n",
    "#         out_1 = F.softmax(self.out_1(x))\n",
    "#         out_2 = F.softmax(self.relu(self.out_2(x)))\n",
    "        out_1 = self.out_1(x)\n",
    "        out_2 = self.out_2(x)\n",
    "#         out_2_new = []\n",
    "#         for i in range(out_2.shape[1]):\n",
    "#             level_2 = cats_level_2[i]\n",
    "#             level_1 = dict_levels[level_2]\n",
    "#             j = dict_level2idx[level_1]\n",
    "#             out_2_new.append(out_2[:, i] * out_1[:, j])\n",
    "#         out_2_new = torch.vstack(out_2_new).T\n",
    "        return out_1, out_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "8880068e-2c15-4771-a4e1-f6d2f689a587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "fc8f0a08-0dd1-433f-9063-c11647cf0489",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outputs, targets):\n",
    "    o1, o2 = outputs\n",
    "    t1, t2 = targets\n",
    "    l1 = nn.CrossEntropyLoss()(o1, t1)\n",
    "    l2 = nn.CrossEntropyLoss()(o2, t2)\n",
    "    loss = 0.1*l1 + 0.9*l2\n",
    "    return l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be477b1b-5de7-41bd-8655-969a9168b2df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "6b2d4c36-d253-4e21-ad17-4e965f04f6b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               epoch: 297\n",
      "\n",
      "                loss:  0.4693 2.5291\n",
      "\n",
      "         hit3_label1:  0.2068 0.2040\n",
      "    precision_label1:  0.0814 0.0842\n",
      "\n",
      "         hit3_label2:  0.9982 0.6317\n",
      "    precision_label2:  0.9599 0.4059\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = TextDataset(X_train_new, y_train_1, y_train_2)\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=1024)\n",
    "\n",
    "val_dataset = TextDataset(X_val_new, y_val_1, y_val_2)\n",
    "val_dataloader = DataLoader(val_dataset, shuffle=False, batch_size=1024)\n",
    "\n",
    "test_dataset = TextDataset(X_test_new, y_test_1, y_test_2)\n",
    "test_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=1024)\n",
    "\n",
    "\n",
    "\n",
    "model = ModelClassification(X_train_new.shape[1]) \n",
    "optimizer = optim.Adam(params=model.parameters(), lr=0.01, weight_decay=1e-5)\n",
    "epochs = 100\n",
    "\n",
    "trainer = Trainer(\n",
    "    model, \n",
    "    loss_func=loss_fn, \n",
    "    opt=optimizer, device=torch.device('cuda'))\n",
    "trainer.train_loop(\n",
    "    train_dataloader, \n",
    "    val_dataloader, \n",
    "    max_epochs=1000, \n",
    "    patience=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "8fa9e87a-208c-4902-ae50-10f45ad9c061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6326732673267327 0.404950495049505\n",
      "0.6326732673267327 0.404950495049505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4592/3649500177.py:8: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  y_pred1 = F.softmax(model(X)[0].cpu().detach()).numpy()\n",
      "/tmp/ipykernel_4592/3649500177.py:9: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  y_pred2 = F.softmax(model(X)[1].cpu().detach()).numpy()\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "batch = next(iter(val_dataloader))\n",
    "X = batch['X'].to(device)\n",
    "y1 = batch['y1'].numpy().argmax(axis=1)\n",
    "y2 = batch['y2'].numpy().argmax(axis=1)\n",
    "\n",
    "y_pred1 = F.softmax(model(X)[0].cpu().detach()).numpy()\n",
    "y_pred2 = F.softmax(model(X)[1].cpu().detach()).numpy()\n",
    "\n",
    "print(hit_at_n(y2, y_pred2, n=3), hit_at_n(y2, y_pred2, n=1))\n",
    "\n",
    "for i in range(y_pred2.shape[1]):\n",
    "    level_2 = cats_level_2[i]\n",
    "    level_1 = dict_levels[level_2]\n",
    "    j = dict_level2idx[level_1]\n",
    "    y_pred2[:, i] = y_pred2[:, i] * y_pred1[:, j]\n",
    "\n",
    "print(hit_at_n(y2, y_pred2, n=3), hit_at_n(y2, y_pred2, n=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "c3fc0133-7557-461e-9c1c-10051b0da36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.629080118694362 0.3837784371909001\n",
      "0.6271018793273986 0.3837784371909001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4592/1611179870.py:8: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  y_pred1 = F.softmax(model(X)[0].cpu().detach()).numpy()\n",
      "/tmp/ipykernel_4592/1611179870.py:9: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  y_pred2 = F.softmax(model(X)[1].cpu().detach()).numpy()\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "batch = next(iter(test_dataloader))\n",
    "X = batch['X'].to(device)\n",
    "y1 = batch['y1'].numpy().argmax(axis=1)\n",
    "y2 = batch['y2'].numpy().argmax(axis=1)\n",
    "\n",
    "y_pred1 = F.softmax(model(X)[0].cpu().detach()).numpy()\n",
    "y_pred2 = F.softmax(model(X)[1].cpu().detach()).numpy()\n",
    "\n",
    "print(hit_at_n(y2, y_pred2, n=3), hit_at_n(y2, y_pred2, n=1))\n",
    "\n",
    "for i in range(y_pred2.shape[1]):\n",
    "    level_2 = cats_level_2[i]\n",
    "    level_1 = dict_levels[level_2]\n",
    "    j = dict_level2idx[level_1]\n",
    "    y_pred2[:, i] = y_pred2[:, i] * y_pred1[:, j]\n",
    "\n",
    "print(hit_at_n(y2, y_pred2, n=3), hit_at_n(y2, y_pred2, n=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625af29d-9d87-4364-9532-da47dcce407b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd42c713-fd96-4f28-94a8-4efb1f73cad2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
